![](_page_0_Picture_0.jpeg)

Available online at www.sciencedirect.com

ScienceDirect

![](_page_0_Picture_3.jpeg)

## Editorial overview: Computational neuroscience as a bridge between artificial intelligence, modeling and data

![](_page_0_Picture_5.jpeg)

# Pietro Verzelli, Tatjana Tchumatchenko and Jeanette Hellgren Kotaleski

Current Opinion in Neurobiology 2024, 84:102835

This review comes from a themed issue on Computational Neuroscience 2023

Edited by Jeanette Hellgreny Kotaleski and Tatjana Tchumatchenko

https://doi.org/10.1016/j.conb.2023.102835

0959-4388/© 2023 Published by Elsevier Ltd.

## Pietro Verzelli

Institute of Experimental Epileptology and Cognition Research, University of Bonn Medical Center, Bonn, Germany

![](_page_0_Picture_14.jpeg)

Pietro Verzelli is a postdoctoral researcher in Tatjana Tchumatchenko's Lab, working on computational and theoretical neuroscience. His research focuses on the relationship between neural activity and behaviors. He received his B.Sc. in Physics from the University of Bologna in 2015 and his M.Sc. in Physics of Complex Systems from the University of Turin in 2017. In his master's thesis, he developed statistical tools to study neural activity, supervised by Prof. Laura Sacerdote. He obtained his PhD at IDSIA under the supervision of Prof. Cesare Alippi and Prof. Lorenzo Livi, with a thesis on machine learning for dynamical systems.

Out of all that we may define (that is to say determine) what that is, which is meant by this word reason when we reckon it amongst the faculties of the mind. For reason, in this sense, is nothing but reckoning (that is, adding and subtracting) of the consequences of general names agreed upon for the marking and signifying of our thoughts.

Leviathan - Thomas Hobbes

#### Introduction

Computational neuroscience continues to be a broad and dynamic discipline that transforms itself as new experimental methods make the collection of new, often multi-modal data types possible. Initially, the focus was on models of single neurons, their ion channels, the statistics of their spikes, and their processing of inputs. New multiunit recording techniques that were possible first in vitro and then gradually in vivo allowed the field to expand into the question of how populations of neurons form circuits that represent sensory stimuli or motor output. Addressing these questions revealed many mathematically and conceptually challenging directions, such as low-dimensional manifolds, their synthesis, and representational meaning. Questions about neural synchrony, phase locking, neural attractors, and their relation to learning, memory, and sleep entered the stage. In the next decades, increased automatization of connectomics studies in organisms such as flies and mice allowed the correlation of the observed activity with the probing of the underlying connectivity. Therefore, neural activity could now, for the first time, be combined with the circuit structure that generated it, enabling detailed insights into the role of cell types in generating specific aspects of neural population activity. New machine-learning methods now enable tracking and segmenting behavior into smaller units while recording neural activity. These methods are complemented by advances in cell profiling and detailed molecular characterization of neural activity and structure, such as quantitative bridges between molecules and behavior, which can be unraveled for the first time. This is an exciting time to be a computational neuroscientist for multiple reasons. First, emerging datasets that cover multiple neural and behavioral levels are emerging and demand new computational approaches. Artificial neural networks can now enable online behavioral segmentation across species and behavioral tasks, but identifying the equivalent of a "grammar" that tells how to use a specific neural activity pattern in a specific region toward the construction of a behavioral trajectory in space is still a challenge. Also, at the level of cellular and molecular mechanisms, new proteomics and connectomics data sets allow unprecedented insights into the learning mechanisms at the single synapse or dendritic branch level, but translating these insights into learning rules in artificial neural networks or revealing their functional correlates in brain circuits remains a fundamental challenge.

## Tatjana Tchumatchenko

Institute of Experimental Epileptology and Cognition Research, University of Bonn Medical Center, Bonn, Germany * Corresponding author: Tchumatchenko, Tatjana

e-mail: tatjana.tchumatchenko@uni-bonn. de

![](_page_1_Picture_4.jpeg)

Prof. Tatjana Tchumatchenko is a professor for Computational Neuroscience of Behavior at the University of Bonn Medical Center in Bonn, Germany. She is a physicist by training and went on to pursue a PhD in Computational Neuroscience in Goettingen University, where she graduated with a 'Summa Cum Laude' in 2011. She completed a postdoctoral fellowship at the Center for Theoretical Neuroscience, Columbia University in New York City from before becoming a Group Leader at the MPI for Brain Research in 2013. Over the years, she has received a number of grants and awards of excellence such as the Heinz Maier-Leibnitz-Prize of the German Research Foundation, Boehringer Ingelheim FENS Research award and the ERC starting grant. In 2017 and 2018 she was elected Steering Committee member for the Bernstein Computational Neuroscience Network and was chosen as one of the 25 young innovators of Germany by Focus magazine in 2018. In 2023, Tatjana was elected to the steering board of the German Neuroscience Society to represent computational neuroscience. Her work investigates how protein dynamics gives rise to synaptic plasticity and how synaptic plasticity contributes to neural circuit computation.

In this issue, computational neuroscientists, whose diverse expertise ranges from molecular interactions to behavioral characterization, share their perspectives on our current knowledge and future challenges and highlight recent breakthroughs. These reviews can be seen as covering five broad areas: 1) machine learning for neuroscience; 2) neurosciences for machine learning; 3) neural computation; 4) computational models; and 5) data analysis methods. Below, we try to map the different contributions to these categories, although several of the reviews fit in more than one.

## Machine learning for neuroscience

The copious data and the intricate nature of neurological phenomena have prompted a profound integration of machine-learning techniques, propelling advancements in this field. Examining how various machine learning ideas have been harnessed unveils a compelling narrative of how these tools have been instrumental in deciphering the complexities of the brain and expanding the frontiers of neuroscientific knowledge. This section intricately explores the contemporary landscape of machine learning's pivotal role in shaping our current understanding of neural systems, emphasizing the incorporation of cutting-edge techniques and their contributions to expanding the frontiers of neuroscientific knowledge.

The article by Farrell, Recanatesi, and Shea-Brown [7] discusses the potential of using neural networks as models for brain function and examines the mechanisms behind neural network behavior. The authors survey the literature and focus on three questions related to how neural circuits change throughout learning. These questions include 1) when networks change their internal representations; 2) when the emerging representations isolate the task at hand; and 3) when and how compressed representations can extract hidden, latent structures of the task or environment.

In their work, Bull and Brunton [4] advocate for an approach to modeling based on modularity. Building parallels with modeling in engineering, they describe an approach relying on modules of different granularities that can be combined to build more complex models. The advantage of using modules is that they can introduce the inductive biases required for making meaningful models while being validated independently, thus reducing the overall optimization cost of the full system. They then describe the system-of-systems approach as a multi-step modeling process that consists of building and validating the modules, assembling them into a larger system, and then co-optimizing them at a system level. This can be achieved using a loss-minimization or reward-driven reinforcement learning approach.

Lindsay [13] focuses on using artificial neural network models to understand how changes in neural circuits or activity produce changes in behavior. This approach can provide causal tests for the relationship between neural and behavioral changes, and interactions with artificial intelligence can offer new insights into the underlying mechanisms.

Modern recording techniques allow studying learning-induced plasticity in large populations of neurons. In their review, Gurnani and Cayco Gajic [9] focus on three main themes: 1) learning as changes to the geometric structure; 2) learning as changes in the dynamics; 3) learning multiple tasks (avoiding catastrophic forgetting). Findings from biological and

#### Jeanette Hellgren Kotaleski

Department of Computer Science, Science for Life Laboratory, KTH Royal Institute of Technology, Stockholm, Sweden Department of Neuroscience, Karolinska Institutet, Stockholm, Sweden

![](_page_2_Picture_3.jpeg)

Jeanette Hellgren Kotaleski is a Professor and team leader at the School of Electrical Engineering and Computer Science at The Royal Institute of Technology in Stockholm (KTH), and she is also affiliated with Dept of Neuroscience at the Karolinska Institute. She has Master Degrees in Medical Science from UmeÃ¥ University and in Engineering Physics from KTH, as well as a PhD in Computer Science from KTH. Her postdoc time was spent at the Fairfax Institute, GMU, under the mentorship of Professor Kim T Blackwell. The main focus of Hellgren KotaleskiâV™s research is to use modeling and simulations to understand the neural mechanisms underlying information processing and learning in the brain in health and disease, with a particular focus on the Basal Ganglia system. The levels of investigation range over multiple biological scales, from large-scale simulations of cellular level neural networks down to kinetic models (i.e. systems biology approaches) of molecular and cellular processes. Professor Hellgren Kotaleski has a long experience in training cross-disciplinary students/postdocs in addition to collaborating closely with experimentalists. She is also working closely with the International Neuroinformatics Coordination Facility, promoting FAIR principles within the neuroscience field. Presently she is also coordinating the building up of EBRAINS-Sweden. EBRAINS is the infrastructure for brain research resulting from the EU flagship project, the Human Brain Project.

artificial networks are presented, shaping a new framework for task learning.

The article by Kalidindi and Crevecoeur [11] explores closed-loop models of movement control and their relevance in understanding how the nervous system translates sensory information into motor commands. Recent research has shown that these models can adapt to changes in environmental parameters, enabling flexible adjustments in actions and decisionmaking. When the task or movement dynamics change, humans employ strategies involving adaptation and controller sensitivity modulation. The article proposes a unified framework for explaining these findings, emphasizing the importance of online estimation of model parameters with dynamic control updates.

#### Neurosciences for machine learning

Since the inception of machine learning, the insights gleaned from neuroscience have served as a wellspring of inspiration. While contemporary machine learning architectures do not explicitly emulate the brain's intricacies, valuable neuroscience discoveries continue to play a pivotal role in advancing the state of the art. This chapter unfolds the nuanced interplay, showcasing how the cross-fertilization of ideas between neuroscience and machine learning has catalyzed innovation and progress in both domains. This section focuses on the utilization of tools derived from neuroscience to enhance the capabilities of machine learning.

In the work by Pache and Rossum [21], the energy cost of learning is addressed. This study highlights the importance of considering the metabolic energy required for acquiring and storing information in biological learning. While current computational models of neural plasticity often overlook this cost, it could be a crucial constraint. The review explores strategies to decrease energy requirements in neural networks and examines how energy efficiency may have influenced biological learning, drawing comparisons with cognitive and neurophysiological findings.

At the intersection of neuroscience and machine learning, there is a growing interest in unsupervised learning, where sensory systems learn from typical inputs without explicit training targets or rewards. Experimental advances allow the exploration of how sensory experience influences neural self-organization and synaptic mechanisms. The review by Matteucci, Piasini, and Zoccolan [16] discusses recent developments in unsupervised and self-supervised learning algorithms, emphasizing their impact on theories of the brain and their potential for breakthroughs in both understanding neural processes and real-world learning machines.

Working memory (WM) is a key function in human cognition, enabling a coherent view of the here and now. This memory system has the ability for one-shot encoding, temporary maintenance, as well as flexible updating and manipulation. WM is affected early on in several neurological and psychiatric conditions, so a better understanding of the underlying neurobiological processes would have high diagnostic and therapeutic value. The theories and mechanisms behind WM are currently being reconsidered, and one hypothesis is based on fast Hebbian synaptic plasticity, in analogy with how long-term memory depends on long-term synaptic potentiation, though on a slower time scale. The article by Lansner, Fiebig, and Herman [12] discusses the experimental and computational arguments for this view. Notably, current mainstream machine learning approaches are mostly missing WM functionality, and there is a possibility that a better understanding of the neurobiology of WM may also contribute to the design of improved ML systems.

Dendrites are fundamental elements in the brain architecture, but their role is often overlooked in artificial neural networks. In Makarov, Pagkalos, and Poirazi [15], the authors discuss the importance of dendrites as computational units in the brain. The brain's dendrites play a crucial role in maximizing efficiency by segregating inputs and integrating them conditionally through nonlinear events, compartmentalizing activity and plasticity, and binding information through synapse clustering. This enables biological networks to process natural stimuli, infer context-specific information, and store it in overlapping populations of neurons. Dendrites contribute to the brain's overall efficiency by employing optimization strategies that balance performance and resource utilization in scenarios with limited energy and space. In Pagkalos, Makarov, and Poirazi [22], the role that dendrites could play in machine learning models is described. Brain-inspired engineering has emerged as a promising approach to overcoming these challenges and designing sustainable AI systems. Innovative solutions have been developed for crucial AI problems such as credit assignment, catastrophic forgetting, and high energy consumption by drawing inspiration from the dendritic mechanisms of biological neurons. These findings offer exciting possibilities for alternative architectures, demonstrating how dendritic research can lead to the development of more powerful and energyefficient artificial learning systems.

## Neural computation

Within the realm of neural computation, the intricacies of the brain's computational prowess manifest in the execution of complex tasks marked by high accuracy and notably low energy consumption, distinguishing it from the resource-intensive demands of silicon-based computers. Despite considerable progress in recent years toward unveiling the mysteries enveloping cognitive phenomena, a substantial portion of these intricacies persists in obscurity. This section serves as a focal point for contributions that address the array of challenges confronted by the brain, engaging in discussions on the potential solutions it employs to navigate and resolve these intricate problems.

In Modirshanechi et al. [19], the authors discuss the role of "surprise" and "novelty" in neuroscience. These two terms have been used in various studies across multiple brain areas and species, but they refer to different things in different studies, raising concerns about whether they relate to the same brain functionalities and mechanisms. They address these concerns by systematically investigating how different aspects of surprise and novelty relate to different brain functions and physiological signals. The authors propose that computational modeling and quantifiable definitions can enable new interpretations of previous findings and serve as a foundation for future studies.

Yoshida and Toyoizumi [26] explore recent theoretical models regarding the role of sleep in learning and memory. The authors discuss the potential benefits of slow waves for enhancing computation by creating distinct neuronal states and propagating traveling waves. They also examine the idea that memory transfer at varying speeds between different brain regions could improve generalization and propose that dreaming may contribute to the formation of efficient representations similar to modern machine learning techniques. The authors suggest further development of these concepts, highlighting the importance of experimental neuroscience and its connection to machine learning.

Micou and O'Leary [18] discuss the changes in neural representations, a phenomenon known as representational drift. Long-term recordings of neural activity have shown that familiar tasks and actions lead to continuous evolution in neural representations, even in the absence of obvious behavioral changes. This drift in neural activity is likely due to continual learning, which can be predicted by neural network models that use iterative learning to optimize weights. Drift can serve as a measurable signal to reveal the precision and effective learning rates of biological plasticity mechanisms at a system level.

In their work, Ashwin, Fadera, and Postlethwaite [1] discuss the modeling of brain dynamics using network attractors. These employ the concept of a "network" within phase space, enabling the modeling of both the discrete states of the system and the permissible transitions between them. This blurs the demarcation between conventional discrete computation and continuous dynamical systems, allowing the modeling of a wide range of phenomena.

The article by Schmidt, Rose, and Muralidharan [24] discusses the changing perspective on neural oscillations, which are now often seen as occurring in transient bursts rather than sustained patterns. This phenomenon is observed across various oscillation frequencies, cognitive tasks, and species. The article reviews recent developments in the analysis of these transient oscillations, computational modeling, and functional roles. Different methods are used to analyze their transient nature and impact on cognitive functions, and computational models are developed to account for the stochastic nature of these oscillations, posing functional constraints.

In their work, Basu and Ito [2] delve into the essential aspects of goal-directed navigation, focusing on how the brain identifies an animal's own location within an environment, primarily through spatially-tuned neurons in the hippocampus and parahippocampal cortices. However, understanding how the brain encodes a remote navigational goal has remained a challenge until recent developments. The study discusses the algorithmic challenges and requirements for forming a representation of such a goal when the animal is not present at the location. It also highlights evidence suggesting that neurons in the orbitofrontal cortex persistently represent a goal location as an animal navigates toward it. Finally, the authors propose a novel perspective on navigation research facilitated by this recently discovered brain's goal map.

### Computational models

The use of computational models is paramount in unraveling the intricate workings of the brain, offering a unique lens through which to understand complex neural phenomena. Modeling serves as a crucial tool for exploring diverse aspects of neuroscience, from the dynamic interactions between neurons and glial cells to the sophisticated signaling networks that underpin cellular operations. This section explores how computational models contribute to our understanding of the brain's complexities across various scales, shedding light on fundamental questions in neuroscience and providing valuable insights into synaptic plasticity, network activity regulation, and the convergence of diverse features within neuronal systems.

In the work by Linne [14], glia cells' role is discussed, focusing on their relevance for neural computation. The improvement of recording techniques has revealed an increasing number of interactions between neurons and glial cells (particularly astrocytes), including development, homeostasis, learning, and more. Even though their role is far from being understood and some of the interactions found experimentally are still controversial, computational models based on these findings have been developed to describe and predict astrocytes' complex functions in the brain.

Signaling networks play a critical role in executing cellular operations, ranging from milliseconds to a potential lifetime. Nearly every aspect of neuronal function, especially at synapses, relies on these signaling pathways. Dysfunction, whether caused by mutations, local dysregulation, or infection, can lead to various diseases, each with diverse phenotypes. The review by Viswan and Bhalla [25] emphasizes the importance of constructing computational models to grapple with the complexity of these pathways. It highlights the benefits of employing families of models at different levels of detail to gain insights into signaling in both health and disease.

Understanding the intricate world of midbrain dopaminergic neurons, a relatively small yet powerful group governing a wide spectrum of behaviors in the mammalian brain, is the focus of the article by Blaess and Krabbe [3]. Recent research has revealed the rich anatomical, molecular, and functional diversity within these neurons, leveraging sophisticated tracing, imaging, transcriptomic, and machine-learning approaches. However, the question remains: how do these diverse features converge to shape functional ensembles within the dopaminergic system? The review delves into recent studies that explore various dimensions of dopaminergic heterogeneity and consider how factors like development, behavior, and disease influence subtype characteristics.

The article by O'Donnell [20] addresses a fundamental question in neuroscience and learning theory: how does the brain reconcile the disparity between rapid neural electrical dynamics and the slower timescales of learning behaviors? The discussion begins with experimental evidence supporting the involvement of slow-timescale factors in synaptic plasticity induction. It then explores potential cellular and synaptic mechanisms, drawing on insights from recent computational models that consider these variables. Ultimately, the article underscores the need for a comprehensive approach that combines experimental and computational studies to unravel the complex interplay between fast and slow plasticity mechanisms in brain learning across different timescales.

The complex role of calcium in regulating neuronal network activity is explored in Jedrzejewska-Szmek, Dorman, and Blackwell [10]. The authors delve into the mechanisms by which calcium directly controls or indirectly regulates critical neuronal functions, particularly synaptic plasticity and ion channel activation, which, in turn, influence neuron spiking activity. Given the technical challenges in obtaining high-resolution experimental data, they underscore the indispensability of computational models in comprehending calcium's intricate dynamics. The models presented offer insights into specific couplings between calcium sources and targets within calcium nanodomains, shedding light on the direction of synaptic plasticity. Additionally, the study considers the complexities arising from cooperativity within calcium domains, raising questions about the preferred computational unit within neurons, with dendritic branches potentially playing a pivotal role.

### Data analysis methods

Over the course of recent years, the field of neuroscience has undergone significant expansion, marked by a surge in both the volume and quality of data amassed, notably through in vivo recordings. This surge not only necessitates but also provides the opportunity to develop innovative approaches for data analysis. Concurrently, the introduction of data science tools has sparked a paradigmatic shift within the discipline. No longer confined to the role of model validation, data analysis has evolved into an active and indispensable mechanism for extracting insights from the intricate fabric of neurophysiological recordings. This section gathers diverse contributions on the compelling intersection of neural computation, offering a comprehensive exploration of the challenges and solutions within this dynamic field. This section collects contributions that explore the nuanced domain of data analysis within neuroscience, offering insights into innovative approaches and methodologies derived from the intricate fabric of neurophysiological recordings.

Esparza, Sebastia´n, and M. de la Prida [6] explore the growing interest in the study of the hippocampal code, which focuses on two approaches: 1) the physiological approach that examines individual cells' contributions based on genetic, biophysical, and circuit factors; and 2) the population-dynamic approach that considers how a large number of neurons represent behavioral variables. The authors suggest that projecting neuronal activity into low-dimensional manifolds can uncover the structure of population representations, but interpreting these manifolds physiologically is challenging. They propose integrating information about behavioral traits, local field potential oscillations, and cell-type specificity into neural manifolds as a strategy to make them more interpretable at the physiological level.

The work by Chiappe [5] highlights the importance of animal movement in understanding the brain's evolution and operation. Precise knowledge of body movement is crucial for functions like motor control, spatial perception, and navigation, which rely on the integration of mechanosensory and visual feedback as well as motorrelated signals. While the circuits responsible for selfmotion estimation and motor-sensory coordination in the central nervous system are still poorly understood, recent advancements have shown that studying Drosophila melanogaster (fruit flies) during naturalistic walking behaviors can provide valuable insights into these fundamental problems with implications for all animals.

The article by Ramaswamy [23] explores the growing importance of data-driven computational models in modern neuroscience. These models encompass neurons, synapses, microcircuits, and mesocircuits, aiming to integrate information from various brain organization levels. The article reviews recent advancements in creating data-driven models of mammalian neural networks in cortical and subcortical areas, discussing the challenges and opportunities. It emphasizes the necessity for interdisciplinary collaborations, model validation, and the potential impact on neuroscience research. The article concludes by highlighting emerging technologies and future directions that promise more comprehensive and predictive data-driven models of brain function and dysfunction.

In their work, Gmaz et al. [8] discuss the evolution of the nervous system's role in enabling navigation and resource pursuit. While newer brain structures brought about increased complexity, they also introduced redundancy. The traditional perspective in movement neuroscience assumes a one-to-one mapping between brain regions and functions. However, recent experimental data challenges this view, suggesting functional redundancy during well-learned and constrained behaviors. This redundancy likely results from bidirectional interactions among various cortical and subcortical structures involved in motor control. The authors argue that these flexible connections allow adaptable interactions across structures depending on behavioral demands. To understand the apparent redundancies in motor control structures, it's essential to observe the system across various actions and behavioral timescales.

McNamee [17] explores the intricate neural microdynamics of cognitive processing within the entorhinal cortex and hippocampus, which are key brain regions for memory, planning, navigation, and imagination. The research reveals complex spatiotemporal patterns in these regions, shedding light on the underlying cognitive computations. The article also proposes a theoretical framework to understand these dynamics better, suggesting that aligning them with models of planning and inference may advance our comprehension of cognitive neural processes.

In summary, the articles published in this issue build bridges between different levels of computational neuroscience, from molecular, circuit, and behavioral levels, and show the wealth of open questions to be addressed in the next few decades.

#### Funding

The authors report no relevant funding sources associated with this article.

## Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

#### Acknowledgements

TT and PV acknowledge the support of the Institute of Experimental Epileptology and Cognition Research at the University of Bonn Medical Center. This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) e Project-ID 227953431 e SFB 1089 (T.T.). JHK acknowledges the support of Swedish Research Council (VR-M-2020-01652), Swedish e-Science Research Center (SeRC), KTH Digital Futures, and EU/Horizon 2020 No. 945539 (HBP SGA3).

#### References

- 1. Ashwin Peter, Fadera Muhammed, Postlethwaite Claire: Network attractors and nonlinear dynamics of neural computation. Curr Opin Neurobiol 2024, 84:102818.
- 2. Basu Raunak, Hiroshi T, Ito: A goal pointer for a cognitive map in the orbitofrontal cortex. Curr Opin Neurobiol 2023, 83: 102803.
- 3. Blaess Sandra, Sabine Krabbe: Cell type specificity for circuit output in the midbrain dopaminergic system. Curr Opin Neurobiol 2023, 88:102811.
- 4. Matthew S, Bull, Bingni W Brunton: Modularity drives generalization in embodied models of brain and behavior. Curr Opin Neurobiol 2023.
- 5. Eugenia Chiappe M: Circuits for selfmotion estimation and walking control in Drosophila. Curr Opin Neurobiol 2023.
- 6. Esparza Julio, Sebastián Enrique R, de la Prida Liset M: From cell types to population dynamics: making hippocampal manifolds physiologically interpretable. Curr Opin Neurobiol 2023, 83:102800.
- 7. Farrell Matthew, Recanatesi Stefano, Shea-Brown Eric: From lazy to rich to exclusive task representations in neural networks and neural codes. Curr Opin Neurobiol 2023, 83:102780.
- 8. Gmaz Jimmie, Keller Jason, T Dudman Joshua, Gallego Juan A: Integrating across behaviors and timescales to understand the neural control of movement. Curr Opin Neurobiol 2023, https://doi.org/10.1016/j.conb.2024.102843.
- 9. Gurnani Harsha, Cayco Gajic N Alex: Signatures of task learning in neural representations. Curr Opin Neurobiol 2023, 83:102759.
- 10. Jedrzejewska-Szmek Joanna, Dorman Daniel B, Blackwell Kim T: Making time and space for calcium control of neuron activity. Curr Opin Neurobiol 2023, 83:102804.
- 11. Hari T, Kalidindi, Crevecoeur Frédéric: Human reaching control in dynamic environments. Curr Opin Neurobiol 2023, 83: 102810.
- 12. Lansner Anders, Fiebig Florian, Herman Pawel: Fast Hebbian plasticity and working memory. Curr Opin Neurobiol 2023, 83: 102809.
- 13. Lindsay Grace W: Grounding neuroscience in behavioral changes using artificial neural networks. Curr Opin Neurobiol 2024, 84:102816.
- 14. Linne Marja-Leena: Computational modeling of neuron-glia interactions in the functioning of neural circuits. Curr Opin Neurobiol 2023.
- 15. Makarov Roman, Pagkalos Michalis, Poirazi Panayiota: Dendrites and efficiency: optimizing performance and resource utilization. Curr Opin Neurobiol 2023, 83:102812.
- 16. Matteucci Giulio, Piasini Eugenio, Zoccolan Davide: Unsupervised learning of mid-level visual representations. Curr Opin Neurobiol 2024, 84:102834.
- 17. McNamee Daniel: The neural microdynamics of cognitive processing. Curr Opin Neurobiol 2023.
- 18. Micou Charles, O'Leary Timothy: Representational drift as a window into neural and behavioral plasticity. Curr Opin Neurobiol 2023, 81:102746.
- 19. Modirshanechi Alireza, Becker Sophia, Johanni Brea, Gerstner Wulfram: Surprise and novelty in the brain. Curr Opin Neurobiol 2023, 82:102758.
- 20. O'Donnell Cian: Nonlinear slowtimescale mechanisms in synaptic plasticity. Curr Opin Neurobiol 2023, 82:102778.
- 21. Pache Aaron, van Rossum Mark CW: Energetically efficient learning in neuronal networks. Curr Opin Neurobiol 2023, 83: 102779.
- 22. Pagkalos Michalis, Makarov Roman, Poirazi Panayiota: Leveraging dendritic properties to advance machine learningand neuro-inspired computing. Curr Opin Neurobiol 2023.
- 23. Ramaswamy Srikanth: Data-driven multiscale computational models of cortical and subcortical regions. Curr Opin Neurobiol 2024, 85:102842.
- 24. Schmidt Robert, Rose Jonas, Muralidharan Vignesh: Transient oscillations as computations for cognition: analysis, modeling and function. Curr Opin Neurobiol 2023, 83:102796.
- 25. Ann Viswan Nisha, Singh Bhalla Upinder: Understanding molecular signaling cascades in neural disease using multiresolution models. Curr Opin Neurobiol 2023, 83:102808.
- 26. Yoshida Kensuke, Toyoizumi Taro: Computational role of sleep in memory reorganization. Curr Opin Neurobiol 2023, 83: 102799.

